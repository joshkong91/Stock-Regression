---
title: "Stocks Data Exploration"
author: "Josh Kong"
date: "8/12/2020"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(corrplot)
library(broom)
stocks <- read_csv("stocks_final.csv")
```


## Looking at the distributions of the data
```{r}
stocks %>% 
  ggplot(aes(Volume)) + 
  geom_histogram()+
  scale_x_log10()

stocks %>% 
  ggplot(aes(Change_perc)) + 
  geom_histogram() 
```


## Exploring the categorical variables
```{r}
#looking at the distribution
stocks %>% 
  count(Country, sort= TRUE)
stocks %>% 
  count(Industry, sort = TRUE)
stocks %>% 
  count(Sector, sort = TRUE)

#for country, I'm going to be looking at USA countries vs every other country
#there are too many different industries. Going to do some feature engineering to bring down the distinct number of industries
#i'm going to leave Sector alone
stocks_category <- stocks%>% 
  separate(Industry, c("Industry", "Other"), extra = "merge", sep = " ", fill = "right") %>% 
  select(Company,Sector, Industry, Country, Change_perc) %>% 
  mutate(Country = fct_lump(Country,1),
         Industry = fct_lump(Industry,10))

#looking at the distributions of the categorical data
stocks_category %>% 
  gather(category,value,-Change_perc,-Company) %>% 
  count(category,value, sort = TRUE) %>% 
  group_by(category) %>% 
  top_n(20,n) %>% 
  ungroup() %>% 
  mutate(value = fct_reorder(value, n)) %>% 
  ggplot(aes(value, n)) + 
  geom_col() + 
  facet_wrap(~category, scale = "free_y") + 
  coord_flip()
  
#taking a closer look at Sector
stocks %>% 
  group_by(Sector) %>% 
  summarise(average_change = mean(Change_perc), count = n()) %>%
  ungroup() %>% 
  mutate(Sector = fct_reorder(Sector,average_change)) %>% 
  ggplot(aes(Sector, average_change, size = count)) + 
  geom_point() +
  coord_flip()
stocks %>% 
  lm(Change_perc ~ Sector, data = .) %>% 
  anova()
#something that we notice is that technology sectors had an drop in their stocks in the month of July
# looks like that Sector is significant in determining the price change


#taking a closer look at industry
stocks_category %>% 
  group_by(Industry) %>% 
  summarise(average_change = mean(Change_perc), count = n()) %>% 
  mutate(Industry = fct_reorder(Industry, average_change)) %>% 
  ggplot(aes(Industry,average_change, size = count)) + 
  geom_point() + 
  coord_flip()
stocks_category %>% 
  lm(Change_perc ~ Industry, data =.) %>% 
  anova()

#taking a closer look at country
stocks_category %>% 
  group_by(Country) %>% 
  summarise(average_change = mean(Change_perc), count = n()) %>% 
  mutate(Country = fct_reorder(Country, average_change)) %>% 
  ggplot(aes(Country,average_change, fill = average_change> 0)) + 
  geom_col() + 
  coord_flip()+
  theme(legend.position = "none")
#appears that US did better than most other countries
stocks_category %>% 
  lm(Change_perc~Country, data = .) %>% 
  anova()
#whether the company was in the US or not is significant in determining the price change of the stocks
```
The three categorical variables that I looked closely at were country, industry and sector.
Looking at the country, it seems that the majority of the companies were from the US, so I decided to make the country column have two options; either they were in the US or not. It seems that the stocks of the companies in the US tend to do better than countries that were not in the US.

After taking a closer look at industry and sector, it does appear that these categorical variables were significant in determining the performance of the stocks.

I'm not going to be using industry for my models due to there being too many different types of industries.

## Exploring the numeric variables
```{r}
#taking a look to see which numeric variables seem significant
#im going to filter for values that have a p-value smaller than 0.05, as they are the significant values
stocks_numeric <- stocks[7:33]
lm(Change_perc ~ ., data = stocks_numeric) %>% 
  tidy(conf.int = TRUE) %>% 
  filter(term != "(Intercept)") %>% 
  arrange(desc(estimate)) %>% 
  mutate(term = fct_reorder(term,estimate)) %>% 
  ggplot(aes(term, estimate)) + 
  geom_point() + 
  coord_flip()+
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high))

#Perf_week, inst_own, P_C, are the ones I will be analyzing

#looking closer at the weekly performance
stocks %>% 
  ggplot(aes(Perf_Week_perc,Change_perc)) + 
  geom_point() + 
  geom_smooth()

#looking at Inst. Own.
stocks %>% 
  ggplot(aes(Inst_Own_perc, Change_perc)) + 
  geom_point()+
  geom_smooth()

#P_C
stocks %>% 
  ggplot(aes(P_C,Change_perc)) + 
  geom_point() + 
  geom_smooth()+ 
  scale_x_log10()

```

## Principle Component Analysis

Due to the large number of numeric variables(27), I decided to use principle component analysis to bring down the number of variables.
```{r}
stocks_pca <- prcomp(stocks_numeric[,-1],scale = TRUE)
summary(stocks_pca)
eig <- (stocks_pca$sdev)^2
vari <- eig*100/sum(eig)
cumvar <- cumsum(vari); cumvar
#going to use the first 13 principle components to get around ~80% of the original variance
stocks_numeric_pca <- stocks_pca$x[,1:13]
```

After using principle component analysis, I reduced the number of numeric variables from 27 to 13 while keeping ~80& of the original variance.

## Creating the machine learning model
Training the data
```{r}
stocks <- cbind(stocks_category[,-c(1,3)], stocks_numeric_pca)
stocks$Sector <- as.factor(stocks$Sector)
set.seed(1)
n <- nrow(stocks)
index <- sample(1:n, round(0.8*n))
train_stocks <- stocks[index,]
test_stocks <- stocks[-index,]
```

Multiple Linear Regression model
```{r}
lin_model <- lm(Change_perc ~ ., data = train_stocks)
summary(lin_model)
par(mfrow = c(2,2))
plot(lin_model)
#model seems to have constant variance and is approximately normal, so the model seems to be valid
```


```{r}
#creating a function that will assess our predictions. I'm going to be using the mean absolute error
MAE <- function(actual,predicted)
{
  mean(abs(actual-predicted))
}
```


Regression tree
```{r}
library(rpart)
library(rpart.plot)
tree_model <- rpart(Change_perc ~ ., data = train_stocks)
rpart.plot(tree_model, digit = 3)
tree_predict <- predict(tree_model, test_stocks)
MAE(test_stocks$Change_perc,tree_predict)
```
Using the regression tree model, I got a mean absolute error of 0.93.


Random Forest Regression model
```{r}
set.seed(12)
library(randomForest)
rf_model <- randomForest(Change_perc ~ ., data = train_stocks, importance = TRUE, mtry = 3, na.action = na.omit,
                         ntree = 500, proximity = TRUE)
rf_predict <- predict(rf_model, test_stocks)
MAE(test_stocks$Change_perc, rf_predict)
```
Using a random forest model, I got a mean absolute error of 0.878.







